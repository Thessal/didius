Author: Web
Title: Sherman McClellan's May 14, 2004 presentation https://www.mcoscillator.com/download/special/McClellan_MTAaward.pdf
Year: 2004
Part I The Development of the McClellan Oscillator
The McClellan Oscillator, and its companion tool the Summation Index, are among the earliest purely technical indicators used by market technicians to decipher the actions of the Advance-Decline Line. These indicators were first created in 1969 by the husband and wife team of Sherman and Marian McClellan. In this article, the McClellans' son Tom explains how the McClellan Oscillator came about, and offers insights into why it is an important tool for market analysis.
What is the McClellan Oscillator?
The McClellan Oscillator is a tool which measures the acceleration in daily Advance-Decline (A-D) statistics by smoothing these numbers with two different exponential moving averages, then finding the difference between them. It became well known among technical analysts, first in Southern California where Sherman and Marian McClellan lived, and later across the United States as word spread about this new tool. But the steps leading to the development of the Oscillator go back many years before its creation in 1969.
A-D Origins
Analyzing Advance-Decline data was first done back in 1926 by Colonel Leonard Ayres, an economist and market analyst working at the Cleveland Trust Company. He wanted to have a different way of looking at the market aside from examining the prices of individual stocks or averages like the Dow Jones Industrial Average. He shared his work with other analysts, including James Hughes who helped pioneer the use of "market breadth" statistics. The weekly financial newspaper Barron's first began publishing Advance-Decline numbers in 1931.
For many years, the most common use of A-D data was to construct a cumulative A-D Line. This is done by computing a running total of each day's value for the "daily breadth", defined as the number of advancing issues minus the number of declining ones. On each successive trading day, the A-D Line changes by the value of the daily breadth. One weakness of this method is that a changing number of issues traded can affect the amplitudes of the movement of the A-D Line, especially when one examines this indicator over long periods of time. One way around this problem is to use a ratio instead of a raw value for A-D. Many analysts do this by taking the daily A-D difference and dividing it by the total of advances plus declines, thereby eliminating the effect of a changing number of issues.
The use of this data did not become widespread, however, until the early 1960s when it was publicized in the writings of Richard Russell (Dow Theory Letters) and Joseph Granville (The Granville Market Letter). One reason for its popularity was that the A-D Line had done such a good job of indicating a divergent top compared to stock prices at the 1961 top, just ahead of a 27% decline in the Dow Jones Industrial Average (DJIA) in 1962.
Haurlan Introduces Exponential Moving Averages
This use of A-D statistics caught the eye of a man named Peter N. Haurlan, who worked as a rocket scientist for the Jet Propulsion Laboratory (JPL) in Pasadena, CA. You may have heard of JPL recently in the context of acting as control center for the Mars rovers. Haurlan also had an interest in stock market technical analysis, and was among the first people to ever use a computer to help him do technical analysis. This is because in the 1960s, he was one of only a handful of people in the world with access to a computer. Haurlan did his day job of plotting trajectories of satellites and other work for JPL, and then at night he would tabulate stock prices and other data from the newspapers and encode that onto IBM punch cards. This way, he could enter and process data into JPL's computer, the only one west of the Mississippi River at the time, during evening hours when it was not being used for work-related purposes. Haurlan was also the first to employ exponential moving averages (EMAs) of price and breadth data. EMAs were a mathematical technique which he had employed for tasks of missile and satellite tracking, and so it seemed an appropriate method to use for tracking the movements of stock prices.
An EMA differs from a simple moving average (SMA) because it weights the more recent data more heavily. An EMA employs a factor known as a "smoothing constant" to give a certain amount of weight to the current period's data. An EMA which uses a 10% smoothing constant, for example, would count today's price or breadth data value as 10% and yesterday's EMA value as 90% for calculating today's new EMA value. Haurlan simplified the terminology by referring to such an EMA as a "10% Trend". A slower EMA which employs a 5% smoothing constant was termed a "5% Trend", and its value would be calculated by adding together 5% of today's price or breadth data and 95% of yesterday's 5% Trend value. Haurlan advocated the use of a variety of different smoothing constant values for stock market analysis, depending on whether one wanted a faster or slower reaction by the EMA. He also recommended using the somewhat round-numbered smoothing constants of 1%, 2%, 5%, 10%, 20%, and 50%, since he knew that most analysts in the 1960s would be doing the math longhand for calculating these EMAs, and the round-numbered smoothing constants made the calculations easier.
A copy of Haurlan's pamphlet, Measuring Trend Values, is included in Part II of this booklet. In it, he outlined the techniques for calculating and interpreting EMAs. In the years since Haurlan introduced these tools, most of the technical analysis community has migrated away from the original terminology, e.g. 10% Trend, because of the public's greater comfort with thinking of moving averages as corresponding to some particular time period. The conversion factor is as follows:
Smoothing Constant = 2 / (n+1) where n is the number of days. Thus, a 19-day EMA equates to a 10% trend as follows: 2 + (19+1) = 2 + 20 = 0.10, or a 10% smoothing constant. Although the rest of the world prefers the reference to a set number of days associated with moving averages, we still employ Haurlan's original terminology. This is partly out of respect for the original work, and partly because using a set number of days for an EMA is misleading. In a Simple Moving Average (SMA), only the data that is contained within the specified lookback period has any influence on the value of the moving average. For the calculation of a 50-day SMA, the 51st day has no voice at all, and the 50th day back has the same voice as yesterday's data. For EMAs, each day of past history still remains in the EMA, but its voice just becomes increasingly fainter, so ascribing a day count label to it does not convey the true nature of what the data are doing within that moving average calculation.
Haurlan's stock market avocation quickly became a new career, and he started the Trade Levels Report newsletter which featured his computer-aided technical analysis using his own newly acquired computer. Sherman and Marian McClellan became aware of this work because of a local business-related TV station operating in Los Angeles known as KWHY. Haurlan's Trade Levels Report was the sponsor of an end-of-market-day chart analysis program called Charting The Market, which was hosted by Gene Morgan.
Using EMAs for a new indicator
By 1969, Haurlan had already employed EMAs for analyzing breadth data. The additional insight that Sherman and Marian McClellan added was to calculate the difference between two different EMAs of the daily breadth figures, a 10% trend and a 5% trend. This provided a different view than looking at the indications from either of these EMAs on their own. Mathematically, this is similar to the Moving Average Convergence-Divergence (MACD) technique developed at about the same time by Gerald Appel, although MACD is usually employed with simple moving averages rather than EMAs.
Coming up with the McClellan Oscillator took the combined talents of both Sherman and Marian McClellan. They met in 1953 during college where Marian was a mathematics major (back when few women chose that major) and Sherman was a business and economics major. Sherman had been taught all of the conventional analysis techniques that every good fundamental analyst should know, but was frustrated because these methods did not work consistently. Accordingly, he turned to technical analysis for greater insights about the market. Back in the 1960s, most stock investors bought stocks for the dividends they paid, not so much for growth prospects. Trading in mutual funds was almost unheard of. So the goal was to buy in such a way as to maximize the dividend yield which one could earn on one's stock purchases. Sherman knew from looking at some price charts that about two or three times a year there were nice price bottoms at which one could buy to help maximize dividend yields, but these bottoms seemed to come at different times of the year, seemingly without any rhythm. He wanted a way to better identify when these bottoms would come, or at least tell when the bottoms were in, and he turned to examining breadth numbers to help with this task.
Sherman and Marian noticed that when the stock market declined sharply, both the 10% trend and the 5% trend of the daily breadth numbers moved to deeply negative levels. During most of an uptrend, these two EMAs would show positive values. The problem was that waiting for an EMA of daily breadth to move from deeply negative to positive meant missing out on the first part of the up move, and that was where much of the price gains were to be made. Seeing the 10% trend move above the 5% trend, even though both were still negative, gave advance warning that a reversal had taken place. Thus, they calculated the difference between the 10% trend and the 5% trend of daily breadth to monitor when such crossovers were taking place. A side benefit of this was that they could detect extremely overbought or oversold conditions when the difference between these EMAs became very large. The McClellans recognized that this new indicator was an "oscillator", because it moved back and forth between extreme values and was neutral at the zero level.
The oscillator could never have seen fruition back in 1969 were it not for the help from Marian. In addition to helping Sherman sort through the logic of the indicators, she was able to do the EMA calculations much more easily due to her math background. Remember that this was back before hand-held calculators were invented, so all of the calculations were done on scratch paper and tallied on ledgers. Charts were created entirely by hand.
When Gene Morgan invited anyone that had developed tools for market analysis to contact him, Sherman was the only one to respond. Morgan coined the name "McClellan Oscillator" to refer to the indicator that the McClellans had created, which became a daily feature on Morgan's TV show. The invitation onto the show led to an introduction to Pete Haurlan, who invited Sherman and Marian McClellan to further publish their work. The booklet Patterns For Profit was the result of this work, and was published originally by Trade Levels in 1970. It included charts with eight years of history of the McClellan Oscillator and Summation Index, all manually calculated by Marian and hand-plotted.
With continued exposure on KWHY-TV and a few seminars, the indicators quickly gained appreciation among technically inclined investors in the Los Angeles viewing area, and word slowly spread across the United States. It gained a wider following in the 1980s after the advent of the personal computer, when early technical analysis programs like Computrac featured the McClellan Oscillator and Summation Index among their packages of technical tools. The McClellans updated their book slightly in 1989, adding text which reflected the fact that calculations could now be done with personal computers. McClellan Financial Publications still sells reprints of this edition along with chart history of the McClellan Oscillator and Summation Index from 1960 forward.
Part III: How Technicians Use The McClellan Oscillator 3
Examination of the chart pattern will give much more information about what the Oscillator has to tell us. Certain chart structures and behavior can be enormously revealing.
Divergences
To the extent that the Oscillator's movements diverge from price action, it can signal an impending change in direction for prices. This is where it helps to understand that the Oscillator serves as an accelerometer for the market breadth statistics. A rocket that is fired into the sky will undergo a deceleration before it reverses direction and starts to fall back to earth, and the same behavior is usually true for stock prices. Measuring the acceleration can be helpful to signal an impending change in trend direction.
The chart below shows several divergences between the price action in the NYSE Composite Index and the McClellan Oscillator. Notice that these divergences tend to occur more often at tops than at bottoms, which is due in part to the way that the US stock market tends to have more rounded tops and exhaustive (spike) bottoms. This is not to say that no divergent bottoms can be found, just that divergent tops are much more frequently seen.
Congestion Zones
A congestion occurs when the Oscillator fluctuates by very small increments over several days. One or two days of small changes is not enough; it has to be a sustained period. The Oscillator value area where a congestion occurs is called a "congestion zone", and they usually form above the zero line. We seldom see them form at extended negative values.
The basic rule to remember is that a congestion zone is something to drop out of. The chart below illustrates a few examples of congestion zones. The common characteristics of each are that they show several days of postings with the Oscillator in a relatively small range, and once the Oscillator breaks down out of that range the market begins to decline sharply. A couple of these examples even have the congestion zone forming at or below zero, but the result was still a drop down out of the congestion zone. Looking at one day's Oscillator value would not convey this information; it takes a chart and someone to interpret that chart to notice behaviors like congestion zones and divergences developing.
Complex Versus Simple Structures
When the Oscillator moves up and down over a period of days on one side of the zero line, we call that a "complex structure". Complexity of a structure implies strength for the side of zero upon which it forms, whether positive or negative. A "simple structure" is one in which the Oscillator crosses zero in one direction in a move lasting from one day up to a few days, and then turns around and heads directly in the opposite direction without forming any complex structure. Simple structures imply weakness for the side upon which they form, although that weakness may not be manifesting itself during the period that the simple structure is formed.
For example, the Oscillator could be chopping up and down below zero, implying that the bears are strong, and then it might pop briefly above zero as the bulls try to regain control. If the Oscillator moves straight up through zero and then turns around and moves straight back down through zero again, it is a sign that the bulls do not really have the strength to carry on their mission for more than a brief period, and the bulls cede control back to the bears.
The chart below shows a few examples of each type of structure. Where a complex structure forms, it implies more strength to come for that side of the market corresponding to the side of the zero line where the structure formed, i.e., complexity above zero is bullish and below zero is bearish. That strength may be temporarily interrupted while the other side tries to exert its influence, but where complexity has formed we have the expectation that more strength will be manifested in that direction. Often we will see trending price moves, either upward or downward, made up of a succession of complex structures that are interrupted only briefly by simple structures. When such a succession of complex structures gives way to a simple structure, it can mean that the trending side of the market is ready to give up control for a while, and the opportunity is there for the other side to pick up the ball. Sometimes, neither side will form a complex structure, meaning that both the bulls and the bears are equally hesitant to take charge.
Oscillator Trendlines
One interesting feature of the Oscillator is that it forms trendlines just like price charts do, but the Oscillator trendlines will usually be broken before the corresponding price trendlines are broken. The next chart shows a few examples of Oscillator trendlines, and in each case the breaking of the trendline signaled a reversal of the prevailing short-term trend. In each case the trendline break in the Oscillator preceded the breaking of the trendlines that could be drawn on the equivalent price points.
It is important to be careful when drawing such lines, and more importantly when drawing conclusions from them. Generally speaking, trendlines which span longer periods become less meaningful, and it is better practice to stick to the steeper trendlines which span 3-6 weeks. As with price trendlines, it is not unusual for the Oscillator to break out above a downtrend line and then go back down to test the top of that line before continuing higher.
Additional Points In Conclusion
The McClellan Oscillator has much to tell us if we are willing to listen. To properly hear the Oscillator's message, one must use a chart of the Oscillator's movements and not just focus on the number.
The Oscillator is based on the daily closing values for the NYSE's totals of advancing and declining issues, and so it does not exist as an intraday indicator. Having said that, it is possible to take the intraday values for the number of advances and declines, and calculate a "what if" value for the Oscillator that assumes those A-D values are the closing ones.
It is also possible to use other data to calculate McClellan Oscillators. We calculate and employ in our analysis breadth versions of the Oscillator which are derived from A-D data on the Nasdaq market, the stocks in the Nasdaq-100 Index, the 30 stocks in the Dow Jones Industrial Average, the corporate bond market, plus a subset of the NYSE breadth data for the "Common Only" stocks (filtering out preferred stocks, rights, warrants, and closed-end funds). It is even possible to create a McClellan Oscillator out of any other breadth statistics you might think of, such as a portfolio of stocks or a subset of the market that includes all of the stocks in a particular sector.
The problem with subset breadth statistics like this is that they tend to all behave in a homogeneous way. In a narrow sector like gold mining or semiconductor stocks, for example, it is typical to see all of them go up one day and then all go down the next day. Other industry groups and sectors show this same effect to a greater or lesser degree. By narrowing the focus to small groups like this, we end up losing the key indication given to us by looking at breadth statistics.
By examining the behavior of a diverse collection of stocks, we can see if there is a different indication from what we see in prices alone. Breadth statistics are valuable because they give some of the best indications about the health of the liquidity that is available to the stock market. A small amount of money can be employed to make a handful of stocks go up or down, and if they are the right stocks then even the major market indices can be moved. But to affect the breadth numbers, which measure all of the stocks on the exchange, requires major changes in the liquidity picture. The available money has to be so plentiful that it can be spread far and wide in order to make the majority of stocks close higher, especially so in order for the market to show positive breadth for several days.
By measuring the acceleration in the breadth statistics, which is what the McClellan Oscillator does, one can gain important insights about impending trend direction changes for prices.
Part IV: McClellan Oscillator Calculation
The standard McClellan Oscillator is calculated as follows: first calculate the daily breadth, which is the difference between the number of advances and the number of declines:
A-D = Advances - Declines
Then calculate two separate exponential moving averages (EMAs), known as the 10 % Trend and the 5 % Trend (so named because of the smoothing constants used in their calculation).
10 % Trend_TODAY = 0.10 * (A-D) + 0.90 * 10 % Trend_YESTERDAY
5 % Trend_TODAY = 0.05 * (A-D) + 0.95 * 5 % Trend_YESTERDAY
The McClellan Oscillator is the difference between these two EMAs:
McOsc = 10 % Trend - 5 % Trend
Many current technical-analysis software packages contain pre-built modules for calculating the McClellan Oscillator. It is also very easy to build in any spreadsheet program. A copy of one such spreadsheet file in Excel format is available at http://www.mcoscillator.com/user/OSC-DATA.xls.
Ratio-Adjusted Oscillator Calculation
The number of stocks traded on the NYSE is constantly changing, and this can affect the amplitudes of indicators that are tied to the number of issues traded, such as the McClellan Oscillator.
To factor out the effect that a changing number of issues has on values of the McClellan Oscillator, we divide the daily breadth number (A-D) by the total of advances plus declines (A + D) to obtain a ratio instead of a raw number. We ignore the number of unchanged issues. We then multiply this ratio by 1000 to bring it back into a "normal" range. In effect, this mathematical step pretends that there are always exactly 1000 stocks traded on the exchange. In formula form:
Ratio-Adjusted A-D = (A-D) / (A + D) * 1000
Once this number is obtained, the rest of the calculations for the EMAs and the Ratio-Adjusted McClellan Oscillator proceed in the same way as for the standard version.
Summation Index
When Sherman and Marian McClellan were first working with the McClellan Oscillator, it occurred to Marian that the "area under the curve" was an additional important feature of this indicator. This stemmed from Marian's background as a mathematician, familiar with the techniques of differential calculus.
To calculate the undulating amount of this area, they added each day's Oscillator value to a running total of all previous Oscillator values, creating the Summation Index. This indicator changes each day by the value of the Oscillator; prolonged positive or negative Oscillator values produce an extended Summation Index.
The chart below shows the Oscillator and Summation Index together and compared to the Dow Jones Industrial Average. When the DJIA is trending upward, we typically see positive Oscillator values and a rising Summation Index. When the Summation Index reaches a high value, it normally falls back as the market consolidates, setting up the market for the next leg of the move.
During their early work with the Summation Index, Sherman and Marian McClellan noticed that this indicator had a total amplitude of about 2000 points. Because all calculations were done manually, they artificially adjusted the Summation Index upward by 1000 points so that its neutral level would be at +1000. This way it would oscillate between 0 and +2000 under normal conditions, and any rare negative reading would signal a truly unique and extreme market condition. The +1000 neutral level for the normal McClellan Summation Index remains the standard to this day.
What was not contemplated in 1970 was the large increase in the number of issues traded on the NYSE, which has led to larger amplitudes of both the Oscillator and the Summation Index. To deal with this issue, one must adjust the values of the Summation Index, either mentally, subjectively factoring chart amplitude changes, or computationally by using Ratio-Adjusted values for the Oscillator and Summation Index.
The Ratio-Adjusted Summation Index (RASI) is still calculated as a running total of all previous Oscillator values, but using Ratio-Adjusted Oscillator values. Rather than the artificial neutral level of +1000 introduced for the conventional Oscillator in 1970, we employ zero as the neutral value for the RASI.
One of the great benefits of the RASI is that it provides intermediate-term (several months) overbought and oversold values. It also indicates when to expect further strength in the major averages and when to expect weakness. If the RASI moves from an oversold reading to above +500, it promises higher highs. The market may undergo a routine correction, but higher highs should ensue once the correction is done. When the RASI fails to get above +500, as shown in the circled instances in the chart above, it signals that further weakness should be expected on the ensuing downtrend.
Part V: Why Breadth Statistics Are Still Important
The earliest citations (Granville, Russell, Harlow) show that the A-D line has been studied for many decades. The earliest citations (Granville 1963, Russell 2004, Harlow 1968) establish a long-standing interest in breadth statistics.
The first criticism of the A-D line is that many NYSE issues are not "real" companies-preferred stocks, rights, warrants, and closed-end funds dilute the signal because they are highly interest-sensitive and drown out votes of operating companies. Historically, the NYSE provided "common-only" data (issues with symbols of three letters or fewer). This data, published daily on the NYSE website and weekly in Barron's, excludes preferred stocks, warrants, rights, bond funds, closed-end stock funds, and country funds.
A more refined approach is to construct a "pure common" A-D line by removing contributions from closed-end funds (CEFs). Figure 7 (not shown) compares the Composite A-D line, the Common-Only A-D line, and the Pure Common A-D line; the two lines are virtually identical, illustrating that CEFs have minimal impact on overall numbers.
Decimalization effects: before 1997 most stocks traded in 1/8-point increments ("ticks"). The NYSE changed to 1/16-point increments in 1997 and fully converted to decimal increments on Jan 29, 2001 (April 9, 2001 for Nasdaq). Previously, an issue had to move at least 1/8 of a point to be counted as advancing or declining; now a single penny suffices. Some analysts claim this increased voting bias makes the A-D line unusable, alleging floor specialists may manipulate closing prices to create tiny gains.
A study of NYSE-supplied closing data from Nov 1 2001 to Mar 5 2004 examined three categories: (1) Composite (all stocks), (2) issues changing more than 1/16 of a point, and (3) issues changing less than 1/16 of a point. The 1/16-point (6.25 cents) threshold assumes that a 7-cent move should be rounded up to the old 1/8-point increment (12.5 cents) and counted as advancing or declining, while moves of 6 cents or less are rounded to unchanged.
Cumulative A-D ratios for the three groups show that the "sub-tick" group (changes <= 1/16 point) has a weaker bias than the market overall; it actually holds back the Composite Cumulative A-D ratio, preventing an even more bullish picture. Thus, decimalization has not artificially inflated breadth statistics.
Problems With Breadth Statistics
The first criticism is that many NYSE issues are not "real" companies (preferred stocks, rights, warrants, closed-end funds). Some analysts attempt to "purify" the A-D line by selecting only stocks they like (e.g., S&P 500 list) or by excluding certain issues. However, determining which stocks to include is difficult; large conglomerates such as General Electric or Danaher resemble mutual-fund holdings, making exclusion problematic.
Another assessment method is to decompose A-D numbers top-down rather than bottom-up. Composite A-D statistics are calculated by data vendors from raw exchange feeds, not directly by the NYSE. The NYSE's "common-only" data excludes any issue with a symbol longer than three letters, removing preferred stocks, warrants, and rights but retaining bond funds, closed-end stock funds, and country funds. Consequently, the Common-Only version usually appears a bit weaker than the Composite version.
To evaluate the impact of closed-end funds (CEFs), historical CEF data were gathered and an A-D line computed for this subset. Subtracting CEF contributions from the Common-Only numbers yields a "Pure Common A-D line." Figure 7 (not shown) demonstrates that the Pure Common line is virtually identical to the Common-Only line, indicating CEFs have negligible effect on overall numbers.
When comparing versions of the A-D line, all lines topped together in August 1989 (point 1) and again later (point 2). The Composite A-D line reached a higher peak at point 2, providing a clearer message about overall market strength, whereas the Common-Only and Pure Common lines topped at lower levels. This suggests the Composite line, with its greater diversity of inputs, is a more reliable indicator.
Undesirable elements (non-operating companies) can still provide valuable information. Daily Common-Only advance-decline numbers subtracted from Composite numbers produce "Uncommon" A-D statistics, which are highly sensitive to interest-rate movements. Comparing the Uncommon A-D line to the Dow Jones Corporate Bond Index shows that a healthy Uncommon line (above its long-term moving average) coincides with strong bond performance, while a weakening Uncommon line predicts weaker bond periods.
Figure 8 (not shown) illustrates the Uncommon A-D line with a 200-day EMA alongside the Dow Jones Corporate Bond Index.
Effects Of Decimalization
The shift to decimal pricing began in 1997 (1/16-point increments) and was fully implemented by the NYSE on Jan 29 2001 (Nasdaq on Apr 9 2001). Previously, a stock needed a price change of at least 1/8 of a point to be counted as advancing or declining; now a single penny suffices. Critics argue that the increased number of "votes" from small-price-change stocks biases the A-D line upward.
A study of NYSE closing data from Nov 1 2001 to Mar 5 2004 examined three issue categories: Composite (all stocks), > 1/16-point price change, and < 1/16-point price change. The 1/16-point (6.25 cents) threshold assumes that a 7-cent move should be rounded up to the old 1/8-point increment (12.5 cents) and counted, while moves of 6 cents or less are treated as unchanged.
Cumulative A-D ratios for the three groups show that the "sub-tick" group (<= 1/16-point changes) has a weaker bias than the market overall, actually holding back the Composite Cumulative A-D ratio. Consequently, decimalization has not artificially created a more bullish A-D line.
In 1872, the NYSE implemented a new system of continuous trading to replace calls of stocks at set times. Under the new system, brokers dealing in a particular stock remain at one location (post) on the trading floor, giving rise to the "specialist" system wherein one independent trader or company manages the auction market in specific securities. Specialists are charged with "making a market" in their specific issue(s), and use their own capital to bridge temporary gaps in supply and demand to reduce price volatility. Because specialists have enormous influence over a stock's price and also have a vested interest in the shares of that issue, there is a concern among many market participants that specialists may have too much power to "rig" the market for their issues. See http://www.nyse.com/gloassary/1042235996028.html.
Overall, while ratio-adjusted A-D statistics have been criticized for decimalization effects, the data indicate that sub-tick price changes have actually restrained the Composite Cumulative A-D ratio, refuting the notion that decimalization has made the A-D line artificially bullish.
Conclusions
(1) The A-D Line is still a useful indicator, but we as analysts must be mindful to listen to exactly what it is telling us instead of denigrating it for not telling us what we think it should.
(2) The very strong A-D statistics seen in 2003-2004 suggest that there should be much more upside to come for stock prices. Ordinary, garden-variety declines may appear from time to time, but the strong breadth suggests that it will be many months before we see a more meaningful decline.
(3) A series of Fed rate hikes beginning now would not result in negative GDP growth because the A-D statistics have been very strong.
(4) The strong breadth statistics among the "Uncommon" or interest-rate-sensitive issues traded on the NYSE tell us that bond prices should continue to move generally higher. Bond prices will still suffer through ordinary corrections, but the strength in bond prices is not yet over.